{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get, Timeout\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import random\n",
    "from datetime import datetime as dt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START SCRAPE\n",
      "************\n",
      "\n",
      "\n",
      "Start 1 page parsing\n",
      "url --> https://quotes.toscrape.com/page/1\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 2 page parsing\n",
      "url --> https://quotes.toscrape.com/page/2\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 3 page parsing\n",
      "url --> https://quotes.toscrape.com/page/3\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 4 page parsing\n",
      "url --> https://quotes.toscrape.com/page/4\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 5 page parsing\n",
      "url --> https://quotes.toscrape.com/page/5\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 6 page parsing\n",
      "url --> https://quotes.toscrape.com/page/6\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 7 page parsing\n",
      "url --> https://quotes.toscrape.com/page/7\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 8 page parsing\n",
      "url --> https://quotes.toscrape.com/page/8\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 9 page parsing\n",
      "url --> https://quotes.toscrape.com/page/9\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "Start 10 page parsing\n",
      "url --> https://quotes.toscrape.com/page/10\n",
      "Status response: 200\n",
      "Start scrape\n",
      "Scrape: OK\n",
      "\n",
      "end content\n",
      "***********\n",
      "END PARSING\n"
     ]
    }
   ],
   "source": [
    "class ParserQuotes:\n",
    "    \n",
    "    url = \"https://quotes.toscrape.com/page/\"\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'user-agent': 'Mozilla / 5.0(Macintosh; Intel Mac OS X 10_14_6)'\n",
    "                    ' AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 98.0 .4758 .102 Safari / 537.36'\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"START SCRAPE\")\n",
    "        print(\"*\" * 12 + \"\\n\")\n",
    "        \n",
    "        self.content = self.start_parsing(self.url, self.headers)\n",
    "\n",
    "\n",
    "    def start_parsing(self, url, headers, start_page=1):\n",
    "        page = start_page\n",
    "        content = []\n",
    "\n",
    "        while True:\n",
    "            page_url = url + str(page)\n",
    "\n",
    "            print()\n",
    "            print(f\"Start {page} page parsing\")\n",
    "            print(\"url --> \" + page_url)\n",
    "\n",
    "            sleep(round(random.uniform(0, 3), 3)) # <<<---( Гигиена )\n",
    "            soup = self.get_page_soup(page_url, headers)\n",
    "            if soup == None:\n",
    "                break\n",
    "\n",
    "            tmp = self.page_scrape(soup)\n",
    "            content.extend(tmp)\n",
    "            try:\n",
    "                page += 1\n",
    "                soup.find('a', href=f\"/page/{page}/\").text # <<<---( Проверка наличия следующей страницы )\n",
    "            except AttributeError:\n",
    "                print()\n",
    "                print(\"end content\")\n",
    "                print(\"***********\")\n",
    "                print(\"END PARSING\")\n",
    "                return content\n",
    "\n",
    "\n",
    "    def get_page_soup(self, page_url, headers):\n",
    "        \n",
    "        try:\n",
    "            response = get(page_url, headers=headers, timeout=10)\n",
    "            print(\"Status response: \" + str(response.status_code))\n",
    "            if response.status_code == 200:\n",
    "                return bs(response.content, \"html.parser\")\n",
    "            else:\n",
    "                print(\"Error: status_code\")\n",
    "                return None\n",
    "        except Timeout:\n",
    "            print(\"error: Превышено время ожидания ответа\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def page_scrape(self, soup):\n",
    "        \n",
    "        print(\"Start scrape\")\n",
    "\n",
    "        res = []\n",
    "        quotes = soup.find_all('div', class_=\"quote\")\n",
    "\n",
    "        for quote in quotes:\n",
    "            quote_dict = {}\n",
    "\n",
    "            quote_dict[\"quote\"] = quote.find('span', class_=\"text\").text\n",
    "            quote_dict[\"author\"] = quote.find('small', class_='author').text\n",
    "\n",
    "            tmp = []\n",
    "            for tag in quote.find_all('a', class_=\"tag\"):\n",
    "                tmp.append(tag.text)\n",
    "            quote_dict[\"tag\"] = tmp\n",
    "\n",
    "            res.append(quote_dict)\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"Scrape: OK\")\n",
    "        return res\n",
    "\n",
    "\n",
    "    def save_content(self):\n",
    "        name_f = \"content_\" + dt.now().strftime('%H:%M_%d.%m.%Y')\n",
    "        with open(name_f, 'w') as f:\n",
    "            json.dump(self.content, f)\n",
    "        \n",
    "        print()\n",
    "        print(\"Save: OK\")\n",
    "        print(\"Name: \" + name_f)\n",
    "\n",
    "\n",
    "test = ParserQuotes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество контента = 100\n",
      "Вот случайная цитата:\n",
      "\n",
      "“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”\n",
      "Terry Pratchett\n",
      "['humor', 'open-mind', 'thinking']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество контента = {len(test.content)}\")\n",
    "\n",
    "print(\"Вот случайная цитата:\\n\")\n",
    "tmp = test.content[random.randint(0, len(test.content) - 1)]\n",
    "print(tmp['quote'])\n",
    "print(tmp['author'])\n",
    "print(tmp['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save: OK\n",
      "Name: content_11:26_12.06.2022\n"
     ]
    }
   ],
   "source": [
    "test.save_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65d7abfdb6b8e36b46552acf763b5fd32417fbc266aa8de01c36d0a0a5eac88a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
